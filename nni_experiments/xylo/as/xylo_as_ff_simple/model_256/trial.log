2023-10-12 17:40:27,253 - main - INFO - PyTorch is currently running on: cuda
2023-10-12 17:40:27,257 - main - INFO - Initial parameters: {'n_population': 256, 'tau_mem': 0.125, 'tau_syn': 0.113}
2023-10-12 17:40:35,259 - main - INFO - Loaded 71464 events from file ../data/4bit_spikeset_PHASE_full.npy
2023-10-12 17:40:35,407 - main - INFO - Generated events partitions: train(42878), val(14293, test(14293))
2023-10-12 17:40:35,459 - main - INFO - Built network: 
TorchSequential  with shape (12, 7) {
    LinearTorch '0_LinearTorch' with shape (12, 256)
    LIFTorch '1_LIFTorch' with shape (256, 256)
    LinearTorch '2_LinearTorch' with shape (256, 7)
    LIFTorch '3_LIFTorch' with shape (7, 7)
}
2023-10-12 17:40:36,848 - main - INFO - Dropped 193 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-10-12 17:40:43,394 - main - INFO - Epoch 0 -> Loss:6.756807841905734, Acc:0.398096970545022, Best Acc:0.398096970545022
2023-10-12 17:41:31,677 - main - INFO - Dropped 145 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-10-12 17:42:25,258 - main - INFO - Dropped 145 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-10-12 17:43:18,992 - main - INFO - Dropped 145 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-10-12 17:44:12,624 - main - INFO - Dropped 145 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-10-12 17:44:58,753 - main - INFO - Dropped 145 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-10-12 17:45:52,270 - main - INFO - Dropped 145 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-10-12 17:46:45,871 - main - INFO - Dropped 145 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-10-12 17:47:39,497 - main - INFO - Dropped 145 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-10-12 17:48:33,095 - main - INFO - Dropped 145 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-10-12 17:49:26,840 - main - INFO - Dropped 145 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-10-12 17:49:32,272 - main - INFO - Epoch 100 -> Loss:0.9929823802738655, Acc:0.6869096760652067, Best Acc:0.7120268662981879
2023-10-12 17:50:20,482 - main - INFO - Dropped 145 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-10-12 17:51:14,049 - main - INFO - Dropped 145 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-10-12 17:52:07,621 - main - INFO - Dropped 145 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-10-12 17:53:01,299 - main - INFO - Dropped 145 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-10-12 17:53:54,867 - main - INFO - Dropped 145 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-10-12 17:54:48,507 - main - INFO - Dropped 145 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-10-12 17:55:42,133 - main - INFO - Dropped 145 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-10-12 17:56:35,734 - main - INFO - Dropped 145 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-10-12 17:57:29,388 - main - INFO - Dropped 145 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-10-12 17:58:23,085 - main - INFO - Dropped 145 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-10-12 17:58:28,489 - main - INFO - Epoch 200 -> Loss:0.893020304237924, Acc:0.7064297208423703, Best Acc:0.7370041278947737
2023-10-12 17:59:16,752 - main - INFO - Dropped 145 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-10-12 18:00:10,435 - main - INFO - Dropped 145 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-10-12 18:01:04,091 - main - INFO - Dropped 145 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-10-12 18:01:57,718 - main - INFO - Dropped 145 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-10-12 18:02:51,338 - main - INFO - Training finished with accuracy: 0.7370041278947737
