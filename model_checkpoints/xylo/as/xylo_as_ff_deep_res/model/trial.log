2023-10-19 21:38:59,076 - main - INFO - PyTorch is currently running on: cuda
2023-10-19 21:38:59,080 - main - INFO - Initial parameters: {'n_population': 256, 'tau_mem': 0.153, 'tau_syn': 0.082}
2023-10-19 21:39:07,095 - main - INFO - Loaded 71464 events from file ../data/wisdm_watch_full_40_encoded.npy
2023-10-19 21:39:07,240 - main - INFO - Generated events partitions: train(42878), val(14293, test(14293))
2023-10-19 21:39:07,294 - main - INFO - Built network: 
TorchSequential  with shape (12, 7) {
    LinearTorch '0_LinearTorch' with shape (12, 256)
    LIFTorch '1_LIFTorch' with shape (256, 256)
    TorchResidual '2_TorchResidual' with shape (256, 256) {
        LinearTorch '0_LinearTorch' with shape (256, 256)
        LIFTorch '1_LIFTorch' with shape (256, 256)
    }
    LinearTorch '3_LinearTorch' with shape (256, 7)
    LIFTorch '4_LIFTorch' with shape (7, 7)
}
2023-10-19 21:39:08,314 - main - INFO - Dropped 193 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-10-19 21:39:08,329 - main - INFO - Dropped 193 output synapses each for 256 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-10-19 21:39:14,863 - main - INFO - Epoch 0 -> Loss:2.0616315690482536, Acc:0.3836143566780941, Best Acc:0.3836143566780941
2023-10-19 21:40:37,549 - main - INFO - Dropped 138 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-10-19 21:40:37,560 - main - INFO - Dropped 188 output synapses each for 191 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-10-19 21:42:13,663 - main - INFO - Dropped 138 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-10-19 21:42:13,674 - main - INFO - Dropped 188 output synapses each for 192 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-10-19 21:43:49,798 - main - INFO - Dropped 138 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-10-19 21:43:49,809 - main - INFO - Dropped 190 output synapses each for 192 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-10-19 21:45:25,866 - main - INFO - Dropped 138 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-10-19 21:45:25,878 - main - INFO - Dropped 190 output synapses each for 192 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-10-19 21:47:01,833 - main - INFO - Dropped 138 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-10-19 21:47:01,844 - main - INFO - Dropped 190 output synapses each for 192 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-10-19 21:48:37,972 - main - INFO - Dropped 138 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-10-19 21:48:37,983 - main - INFO - Dropped 190 output synapses each for 192 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-10-19 21:50:14,494 - main - INFO - Dropped 138 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-10-19 21:50:14,505 - main - INFO - Dropped 190 output synapses each for 192 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-10-19 21:51:51,143 - main - INFO - Dropped 138 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-10-19 21:51:51,156 - main - INFO - Dropped 190 output synapses each for 192 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-10-19 21:53:27,300 - main - INFO - Dropped 138 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-10-19 21:53:27,311 - main - INFO - Dropped 190 output synapses each for 192 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-10-19 21:55:03,427 - main - INFO - Dropped 138 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-10-19 21:55:03,440 - main - INFO - Dropped 190 output synapses each for 192 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-10-19 21:55:13,040 - main - INFO - Epoch 100 -> Loss:0.6814981800753895, Acc:0.751346813125306, Best Acc:0.7586231022178689
2023-10-19 21:56:31,772 - main - INFO - Dropped 138 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-10-19 21:56:31,783 - main - INFO - Dropped 190 output synapses each for 192 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-10-19 21:58:08,181 - main - INFO - Dropped 138 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-10-19 21:58:08,192 - main - INFO - Dropped 190 output synapses each for 192 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-10-19 21:59:44,513 - main - INFO - Dropped 138 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-10-19 21:59:44,526 - main - INFO - Dropped 190 output synapses each for 192 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-10-19 22:01:21,019 - main - INFO - Dropped 138 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-10-19 22:01:21,031 - main - INFO - Dropped 190 output synapses each for 192 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-10-19 22:02:57,212 - main - INFO - Dropped 138 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-10-19 22:02:57,223 - main - INFO - Dropped 190 output synapses each for 192 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-10-19 22:04:33,485 - main - INFO - Dropped 138 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-10-19 22:04:33,496 - main - INFO - Dropped 190 output synapses each for 192 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-10-19 22:06:09,710 - main - INFO - Dropped 138 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-10-19 22:06:09,721 - main - INFO - Dropped 190 output synapses each for 192 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-10-19 22:07:46,055 - main - INFO - Dropped 138 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-10-19 22:07:46,068 - main - INFO - Dropped 190 output synapses each for 192 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-10-19 22:09:22,235 - main - INFO - Dropped 138 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-10-19 22:09:22,247 - main - INFO - Dropped 191 output synapses each for 192 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-10-19 22:10:58,655 - main - INFO - Dropped 138 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-10-19 22:10:58,666 - main - INFO - Dropped 191 output synapses each for 192 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-10-19 22:11:08,238 - main - INFO - Epoch 200 -> Loss:0.6186603075120507, Acc:0.7691177499475267, Best Acc:0.7748548240397397
2023-10-19 22:12:26,437 - main - INFO - Dropped 138 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-10-19 22:12:26,449 - main - INFO - Dropped 191 output synapses each for 192 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-10-19 22:14:02,672 - main - INFO - Dropped 138 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-10-19 22:14:02,683 - main - INFO - Dropped 191 output synapses each for 192 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-10-19 22:15:39,241 - main - INFO - Dropped 138 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-10-19 22:15:39,253 - main - INFO - Dropped 191 output synapses each for 192 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-10-19 22:17:15,545 - main - INFO - Dropped 138 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-10-19 22:17:15,558 - main - INFO - Dropped 191 output synapses each for 192 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-10-19 22:18:51,706 - main - INFO - Training finished with accuracy: 0.7825508990414888
