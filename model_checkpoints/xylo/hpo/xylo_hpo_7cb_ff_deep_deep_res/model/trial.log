2023-11-08 13:42:04,604 - main - INFO - PyTorch is currently running on: cuda
2023-11-08 13:42:04,607 - main - INFO - Initial parameters: {'n_population': 256, 'tau_mem': 0.094, 'tau_syn': 0.045}
2023-11-08 13:42:12,643 - main - INFO - Loaded 71464 events from file ../data/wisdm_watch_full_40_encoded.npy
2023-11-08 13:42:12,790 - main - INFO - Generated events partitions: train(42878), val(14293, test(14293))
2023-11-08 13:42:12,845 - main - INFO - Built network: 
TorchSequential  with shape (12, 7) {
    LinearTorch '0_LinearTorch' with shape (12, 256)
    LIFTorch '1_LIFTorch' with shape (256, 256)
    TorchResidual '2_TorchResidual' with shape (256, 256) {
        LinearTorch '0_LinearTorch' with shape (256, 256)
        LIFTorch '1_LIFTorch' with shape (256, 256)
    }
    TorchResidual '3_TorchResidual' with shape (256, 256) {
        LinearTorch '0_LinearTorch' with shape (256, 256)
        LIFTorch '1_LIFTorch' with shape (256, 256)
    }
    LinearTorch '4_LinearTorch' with shape (256, 7)
    LIFTorch '5_LIFTorch' with shape (7, 7)
}
2023-11-08 13:42:14,273 - main - INFO - Dropped 193 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-11-08 13:42:14,290 - main - INFO - Dropped 193 output synapses each for 256 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-11-08 13:42:14,306 - main - INFO - Dropped 193 output synapses each for 256 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-11-08 13:42:29,516 - main - INFO - Epoch 0 -> Loss:1.945911169052124, Acc:0.1467151752606171, Best Acc:0.1467151752606171
2023-11-08 13:44:37,592 - main - INFO - Dropped 146 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-11-08 13:44:37,602 - main - INFO - Dropped 40 output synapses each for 172 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-11-08 13:44:37,613 - main - INFO - Dropped 59 output synapses each for 182 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-11-08 13:47:00,352 - main - INFO - Dropped 146 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-11-08 13:47:00,362 - main - INFO - Dropped 51 output synapses each for 179 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-11-08 13:47:00,373 - main - INFO - Dropped 80 output synapses each for 192 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-11-08 13:49:23,125 - main - INFO - Dropped 146 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-11-08 13:49:23,135 - main - INFO - Dropped 59 output synapses each for 182 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-11-08 13:49:23,147 - main - INFO - Dropped 99 output synapses each for 201 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-11-08 13:51:45,932 - main - INFO - Dropped 146 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-11-08 13:51:45,943 - main - INFO - Dropped 73 output synapses each for 187 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-11-08 13:51:45,955 - main - INFO - Dropped 115 output synapses each for 206 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-11-08 13:54:08,303 - main - INFO - Dropped 146 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-11-08 13:54:08,314 - main - INFO - Dropped 76 output synapses each for 190 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-11-08 13:54:08,326 - main - INFO - Dropped 119 output synapses each for 209 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-11-08 13:56:30,861 - main - INFO - Dropped 146 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-11-08 13:56:30,872 - main - INFO - Dropped 77 output synapses each for 191 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-11-08 13:56:30,884 - main - INFO - Dropped 121 output synapses each for 210 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-11-08 13:58:53,268 - main - INFO - Dropped 146 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-11-08 13:58:53,279 - main - INFO - Dropped 80 output synapses each for 191 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-11-08 13:58:53,291 - main - INFO - Dropped 123 output synapses each for 210 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-11-08 14:01:15,773 - main - INFO - Dropped 146 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-11-08 14:01:15,784 - main - INFO - Dropped 82 output synapses each for 191 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-11-08 14:01:15,796 - main - INFO - Dropped 125 output synapses each for 212 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-11-08 14:03:38,223 - main - INFO - Dropped 146 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-11-08 14:03:38,235 - main - INFO - Dropped 84 output synapses each for 191 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-11-08 14:03:38,249 - main - INFO - Dropped 128 output synapses each for 212 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-11-08 14:06:00,355 - main - INFO - Dropped 146 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-11-08 14:06:00,366 - main - INFO - Dropped 85 output synapses each for 191 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-11-08 14:06:00,378 - main - INFO - Dropped 130 output synapses each for 212 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-11-08 14:06:14,693 - main - INFO - Epoch 100 -> Loss:1.5364586928995645, Acc:0.4081718323654936, Best Acc:0.42090533827747845
2023-11-12 00:33:27,783 - main - INFO - PyTorch is currently running on: cuda
2023-11-12 00:33:27,788 - main - INFO - Initial parameters: {'n_population': 256, 'tau_mem': 0.178, 'tau_syn': 0.078}
2023-11-12 00:33:35,780 - main - INFO - Loaded 71464 events from file ../data/wisdm_watch_full_40_encoded.npy
2023-11-12 00:33:35,928 - main - INFO - Generated events partitions: train(42878), val(14293, test(14293))
2023-11-12 00:33:35,980 - main - INFO - Built network: 
TorchSequential  with shape (12, 7) {
    LinearTorch '0_LinearTorch' with shape (12, 256)
    LIFTorch '1_LIFTorch' with shape (256, 256)
    TorchResidual '2_TorchResidual' with shape (256, 256) {
        LinearTorch '0_LinearTorch' with shape (256, 256)
        LIFTorch '1_LIFTorch' with shape (256, 256)
    }
    TorchResidual '3_TorchResidual' with shape (256, 256) {
        LinearTorch '0_LinearTorch' with shape (256, 256)
        LIFTorch '1_LIFTorch' with shape (256, 256)
    }
    LinearTorch '4_LinearTorch' with shape (256, 7)
    LIFTorch '5_LIFTorch' with shape (7, 7)
}
2023-11-12 00:33:37,406 - main - INFO - Dropped 193 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-11-12 00:33:37,429 - main - INFO - Dropped 193 output synapses each for 256 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-11-12 00:33:37,447 - main - INFO - Dropped 193 output synapses each for 256 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-11-12 00:33:52,777 - main - INFO - Epoch 0 -> Loss:2.093175617659964, Acc:0.1467151752606171, Best Acc:0.1467151752606171
2023-11-12 00:36:01,051 - main - INFO - Dropped 150 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-11-12 00:36:01,064 - main - INFO - Dropped 185 output synapses each for 207 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-11-12 00:36:01,078 - main - INFO - Dropped 188 output synapses each for 254 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-11-12 00:38:23,741 - main - INFO - Dropped 150 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-11-12 00:38:23,753 - main - INFO - Dropped 187 output synapses each for 207 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-11-12 00:38:23,767 - main - INFO - Dropped 191 output synapses each for 254 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-11-12 00:40:46,597 - main - INFO - Dropped 150 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-11-12 00:40:46,609 - main - INFO - Dropped 189 output synapses each for 207 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-11-12 00:40:46,623 - main - INFO - Dropped 192 output synapses each for 254 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-11-12 00:43:08,910 - main - INFO - Dropped 150 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-11-12 00:43:08,922 - main - INFO - Dropped 190 output synapses each for 207 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-11-12 00:43:08,946 - main - INFO - Dropped 192 output synapses each for 254 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-11-12 00:45:31,449 - main - INFO - Dropped 150 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-11-12 00:45:31,461 - main - INFO - Dropped 190 output synapses each for 207 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-11-12 00:45:31,475 - main - INFO - Dropped 192 output synapses each for 255 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-11-12 00:47:53,481 - main - INFO - Dropped 150 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-11-12 00:47:53,493 - main - INFO - Dropped 190 output synapses each for 207 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-11-12 00:47:53,507 - main - INFO - Dropped 192 output synapses each for 256 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-11-12 00:50:15,179 - main - INFO - Dropped 150 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-11-12 00:50:15,191 - main - INFO - Dropped 190 output synapses each for 207 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-11-12 00:50:15,205 - main - INFO - Dropped 192 output synapses each for 256 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-11-12 00:52:37,249 - main - INFO - Dropped 150 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-11-12 00:52:37,261 - main - INFO - Dropped 190 output synapses each for 207 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-11-12 00:52:37,275 - main - INFO - Dropped 192 output synapses each for 256 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-11-12 00:54:58,943 - main - INFO - Dropped 150 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-11-12 00:54:58,955 - main - INFO - Dropped 190 output synapses each for 207 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-11-12 00:54:58,969 - main - INFO - Dropped 192 output synapses each for 256 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-11-12 00:57:20,764 - main - INFO - Dropped 150 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-11-12 00:57:20,776 - main - INFO - Dropped 190 output synapses each for 207 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-11-12 00:57:20,790 - main - INFO - Dropped 192 output synapses each for 256 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-11-12 00:57:35,012 - main - INFO - Epoch 100 -> Loss:0.6880289941299252, Acc:0.7146855103897012, Best Acc:0.7603722101728119
2023-11-12 00:59:42,565 - main - INFO - Dropped 150 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-11-12 00:59:42,579 - main - INFO - Dropped 190 output synapses each for 207 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-11-12 00:59:42,595 - main - INFO - Dropped 192 output synapses each for 256 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-11-12 01:02:04,343 - main - INFO - Dropped 150 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-11-12 01:02:04,355 - main - INFO - Dropped 190 output synapses each for 207 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-11-12 01:02:04,369 - main - INFO - Dropped 192 output synapses each for 256 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-11-12 01:04:26,169 - main - INFO - Dropped 150 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-11-12 01:04:26,181 - main - INFO - Dropped 190 output synapses each for 207 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-11-12 01:04:26,195 - main - INFO - Dropped 192 output synapses each for 256 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-11-12 01:06:48,073 - main - INFO - Dropped 150 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-11-12 01:06:48,085 - main - INFO - Dropped 191 output synapses each for 207 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-11-12 01:06:48,099 - main - INFO - Dropped 192 output synapses each for 256 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-11-12 01:09:09,592 - main - INFO - Dropped 150 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-11-12 01:09:09,604 - main - INFO - Dropped 191 output synapses each for 207 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-11-12 01:09:09,618 - main - INFO - Dropped 192 output synapses each for 256 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-11-12 01:11:21,866 - main - INFO - Dropped 150 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-11-12 01:11:21,878 - main - INFO - Dropped 191 output synapses each for 207 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-11-12 01:11:21,893 - main - INFO - Dropped 192 output synapses each for 256 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-11-12 01:13:43,407 - main - INFO - Dropped 150 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-11-12 01:13:43,420 - main - INFO - Dropped 191 output synapses each for 207 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-11-12 01:13:43,436 - main - INFO - Dropped 192 output synapses each for 256 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-11-12 01:16:05,283 - main - INFO - Dropped 150 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-11-12 01:16:05,295 - main - INFO - Dropped 191 output synapses each for 207 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-11-12 01:16:05,309 - main - INFO - Dropped 192 output synapses each for 256 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-11-12 01:18:27,082 - main - INFO - Dropped 150 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-11-12 01:18:27,094 - main - INFO - Dropped 191 output synapses each for 207 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-11-12 01:18:27,108 - main - INFO - Dropped 192 output synapses each for 256 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-11-12 01:20:48,775 - main - INFO - Dropped 150 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-11-12 01:20:48,788 - main - INFO - Dropped 191 output synapses each for 207 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-11-12 01:20:48,802 - main - INFO - Dropped 192 output synapses each for 256 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-11-12 01:21:02,979 - main - INFO - Epoch 200 -> Loss:0.5763658910262875, Acc:0.7608619604001958, Best Acc:0.787238508360736
2023-11-12 01:23:10,794 - main - INFO - Dropped 150 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-11-12 01:23:10,806 - main - INFO - Dropped 191 output synapses each for 207 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-11-12 01:23:10,820 - main - INFO - Dropped 192 output synapses each for 256 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-11-12 01:25:32,624 - main - INFO - Dropped 150 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-11-12 01:25:32,636 - main - INFO - Dropped 191 output synapses each for 207 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-11-12 01:25:32,651 - main - INFO - Dropped 192 output synapses each for 256 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-11-12 01:27:54,573 - main - INFO - Dropped 150 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-11-12 01:27:54,586 - main - INFO - Dropped 192 output synapses each for 207 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-11-12 01:27:54,602 - main - INFO - Dropped 192 output synapses each for 256 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-11-12 01:30:16,352 - main - INFO - Dropped 150 output synapses each for 12 neurons in layer LinearTorch '0_LinearTorch' with shape (12, 256) (Rec:None)
2023-11-12 01:30:16,365 - main - INFO - Dropped 192 output synapses each for 207 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-11-12 01:30:16,382 - main - INFO - Dropped 192 output synapses each for 256 neurons in layer LinearTorch '0_LinearTorch' with shape (256, 256) (Rec:None)
2023-11-12 01:32:38,064 - main - INFO - Training finished with accuracy: 0.7936052613167285
